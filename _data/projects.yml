- layout: top-middle
  name: Dali Renderer
  link: github.com/austinjones/dali-rs
  # github: austinjones/dali-rs
  # quote: >
  #   This is probably one of the greatest apps ever created, if you don't agree you're probably wrong.
  description: > # this will include new lines to allow paragraphs
    Generating quality images for print on canvas is suprisingly difficult.  To avoid 'pixelation', a 20x20" 
    canvas requires 8k x 8k resolution (or more).  For large pieces (40" to 60") you need extreme resolution.


    At these resolutions, performance is critical.  Dali is a GPU rendering library that leverages texture 
    sampling & fragment shaders to generate rich texture at these resolutions.  It can generate instant previews, 
    and production renders in a few minutes.


    To see it 'in production', see one of [my works](https://austinjones.onfabrik.com/portfolio/texture-from-nothing).

- layout: top-middle
  name: Normalizing Flows
  link: github.com/austinjones/normalizing-flows
  # github: austinjones/dali-rs
  # quote: >
  #   An exploration of Normalizing Flows
  description: > # this will include new lines to allow paragraphs
    My interest in multi-dimensional density estimation began when I started developing methods to model color palettes 
    as continuous density, not just a fixed list of colors.  After exploring many techniques, Normalizing Flows emerged 
    as a promising technique with many uses during creative generation.


    I implemented several types of Normalizing Flows in Pytorch and Swift for Tensorflow, and applied them to synthetic & real-world problems.
    

    I developed a method for benchmarking Flow performance.  The Normalizing Flow objective minimizes 
    the KL divergence between the data distribution and the normalized data (under the change of variables theorem).  In real-world problems, 
    the probability density of the data is not tractable.  However, if you choose source data with an analytic density function, you can
    generate a monte-carlo estimate of the KL divergence.


    This method allows the rigorous benchmarking of model performance, and performance measurement at **any** layer.